将Transformer从原来自己写的代码换成pytorch的官方代码后

CNN_Transformer和CNN_ML_Transformer直接扑街，SET和SET_official的验证集准确率降到了85%左右。Transformer_TIM展现了卓越的性能，验证集准确率能稳在95%左右，学习率可能为1e-3

当用Transformer_TIM训练IEMOCAP数据集时，需要注意将学习率降低至1e-4左右，防止模型准确率大幅震荡，中间验证集准确率有两次大幅下降，

单纯使用TIM网络训练出来的结果一开始波动很大，但是之后会稳定下来，效果也不错

用Transformer也能训练出不错的效果，最后能收敛到95%左右。但是训练时20多个epoch会训练集和验证集的准确率会有一次较大的下降，60个epoch的时候也会有一次较大的下降（95%->86%）

去掉SET_official中的位置编码，会导致刚开始训练时模型在验证集上的准确率变化幅度大，但会最后会平稳到一个更大的值（加上位置编码(86%)，不加位置编码90%）

可不可以将原来的weight-layer直接换成注意力机制，每一层的输出可以看成一个head 将级联后的输出通过三个卷积层(将39维变成64维(maybe))变成q,k,v : 经过测试，实际效果和原来相比会略差一点。注意要提高多头注意力中单头注意力的维度`d_qkv`，加上前馈层可以明显提升性能和波动范围。

下一级的TAB输出减去上一级的TAB输出（差分）再加上经过1×1的卷积的输出以及最后一个TAB的输出，经过weight layer，或者直接用上所有的TAB输出：与第一种相比，第二种比第一种的准确率提升有限但是会更加稳定，其实只需要将7个差分加上最后一个TAB的输出即可达到相当不错的结果（同时比较稳定）

| 模型名 | 序号 | 学习率 | 迭代次数 | 准确率        |      |      |      |      | 备注 |
| ------ | ---- | ------ | -------- | ------------- | ---- | ---- | ---- | ---- | ---- |
| TT0    | 1    | 2e-4   | 60       | [100, 99.718] |      |      |      |      | 临时 |
|        |      |        |          |               |      |      |      |      |      |
|        |      |        |          |               |      |      |      |      |      |

降低初始学习率会让模型稳定，但是准确率下降，增加初始学习率会让模型波动幅度变大，但是能达到更高的准确率
Transformer_DeltaTIM可以更快地收敛到最高的值
如果使用高斯混合聚类对特征进行聚类，学习率要调得大一点，并且用8个高斯混合聚类模型的效果可能比用12个的效果好
用Transformer_DeltaTIM训练IEMOCAP数据集时，要用比较小的学习率如1e-4，并且scheduler的step_size改小一点(20)，gamma改大一点(0.4)，同时加上正则化(0.3),dropout(0.2)

使用IEMOCAP的预训练模型，微调transformer部分，最后能达到训练集100%，验证集99.275%， 测试集99.719%，感觉差别不大，效果不明显。

后面尝试将BN换成LN（效果不好），将Transformer中的Relu换成Gelu

TIM中残差连接用的是相乘的方式，该方式会影响到验证集上的稳定性，换成相加会更加稳定，但是性能会更差。

对于AT_DeltaTIM模型，在MODMA_V1数据集中，将TIM的残差连接方式改为相加，增大weight_decay(0.4)可以一定程度上提高性能（训练集98%，验证集95%）

加载预训练模型时，只需要模型结构相同即可，在forward()中进行的操作一般并不太重要。

对于AT_DeltaTIM模型，在IEMOCAP_V2数据集中，将TIM的残差连接方式改为相加，增大weight_decay(0.5)，提高学习率至5e-4可以一定程度上提高性能（训练集96.613%，验证集98.509%）；在IEMOCAP_V1数据集中，在之前的设置基础上将每次学习率改变的步长从25改为30（训练集 98.068%，测试集92.853%）

对于AT_DeltaTIM模型，使用IEMOCAP上预训练的模型，将预训练时使用的相加改为相乘，对于模型而言其实无明显增益（训练集：99.879  验证集：98.590），但是变得更加稳定了（有时会不稳定）

对于TAB使用1x1卷积+2x2卷积+1x1卷积代替两个2x2卷积，性能会有一定的衰减（大概2%），但会会比较稳定。

AT_DeltaTIM_v2相较于AT_DeltaTIM性能更好一点
