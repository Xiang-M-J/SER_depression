将Transformer从原来自己写的代码换成pytorch的官方代码后

CNN_Transformer和CNN_ML_Transformer直接扑街，SET和SET_official的验证集准确率降到了85%左右。Transformer_TIM展现了卓越的性能，验证集准确率能稳在95%左右，学习率可能为1e-3

当用Transformer_TIM训练IEMOCAP数据集时，需要注意将学习率降低至1e-4左右，防止模型准确率大幅震荡，中间验证集准确率有两次大幅下降，

单纯使用TIM网络训练出来的结果一开始波动很大，但是之后会稳定下来，效果也不错

用Transformer也能训练出不错的效果，最后能收敛到95%左右。但是训练时20多个epoch会训练集和验证集的准确率会有一次较大的下降，60个epoch的时候也会有一次较大的下降（95%->86%）

去掉SET_official中的位置编码，会导致刚开始训练时模型在验证集上的准确率变化幅度大，但会最后会平稳到一个更大的值（加上位置编码(86%)，不加位置编码90%）

可不可以将原来的weight-layer直接换成注意力机制，每一层的输出可以看成一个head 将级联后的输出通过三个卷积层(将39维变成64维(maybe))变成q,k,v  经过测试，实际效果和原来相比会略差一点。注意要提高多头注意力中单头注意力的维度`d_qkv`，加上前馈层可以明显提升性能和波动范围。

| 模型名 | 序号 | 学习率 | 迭代次数 | 准确率        |      |      |      |      | 备注 |
| ------ | ---- | ------ | -------- | ------------- | ---- | ---- | ---- | ---- | ---- |
| TT0    | 1    | 2e-4   | 60       | [100, 99.718] |      |      |      |      | 临时 |
|        |      |        |          |               |      |      |      |      |      |
|        |      |        |          |               |      |      |      |      |      |

