将Transformer从原来自己写的代码换成pytorch的官方代码后

CNN_Transformer和CNN_ML_Transformer直接扑街，SET和SET_official的验证集准确率降到了85%左右。Transformer_TIM展现了卓越的性能，验证集准确率能稳在95%左右，学习率可能为1e-3

当用Transformer_TIM训练IEMOCAP数据集时，需要注意将学习率降低至1e-4左右，防止模型准确率大幅震荡，中间验证集准确率有两次大幅下降，

单纯使用TIM网络训练出来的结果一开始波动很大，但是之后会稳定下来，效果也不错

用Transformer也能训练出不错的效果，最后能收敛到95%左右。但是训练时20多个epoch会训练集和验证集的准确率会有一次较大的下降，60个epoch的时候也会有一次较大的下降（95%->86%）

去掉SET_official中的位置编码，会导致刚开始训练时模型在验证集上的准确率变化幅度大，但会最后会平稳到一个更大的值（加上位置编码(86%)，不加位置编码90%）

可不可以将原来的weight-layer直接换成注意力机制，每一层的输出可以看成一个head 将级联后的输出通过三个卷积层(将39维变成64维(maybe))变成q,k,v : 经过测试，实际效果和原来相比会略差一点。注意要提高多头注意力中单头注意力的维度`d_qkv`，加上前馈层可以明显提升性能和波动范围。

下一级的TAB输出减去上一级的TAB输出（差分）再加上经过1×1的卷积的输出以及最后一个TAB的输出，经过weight layer，或者直接用上所有的TAB输出：与第一种相比，第二种比第一种的准确率提升有限但是会更加稳定，其实只需要将7个差分加上最后一个TAB的输出即可达到相当不错的结果（同时比较稳定）

| 模型名 | 序号 | 学习率 | 迭代次数 | 准确率        |      |      |      |      | 备注 |
| ------ | ---- | ------ | -------- | ------------- | ---- | ---- | ---- | ---- | ---- |
| TT0    | 1    | 2e-4   | 60       | [100, 99.718] |      |      |      |      | 临时 |
|        |      |        |          |               |      |      |      |      |      |
|        |      |        |          |               |      |      |      |      |      |

降低初始学习率会让模型稳定，但是准确率下降，增加初始学习率会让模型波动幅度变大，但是能达到更高的准确率
Transformer_DeltaTIM可以更快地收敛到最高的值
如果使用高斯混合聚类对特征进行聚类，学习率要调得大一点，并且用8个高斯混合聚类模型的效果可能比用12个的效果好
用Transformer_DeltaTIM训练IEMOCAP数据集时，要用比较小的学习率如1e-4，并且scheduler的step_size改小一点(20)，gamma改大一点(0.4)，同时加上正则化(0.3),dropout(0.2)

使用IEMOCAP的预训练模型，微调transformer部分，最后能达到训练集100%，验证集99.275%， 测试集99.719%，感觉差别不大，效果不明显。

后面尝试将BN换成LN（效果不好），将Transformer中的Relu换成Gelu

TIM中残差连接用的是相乘的方式，该方式会影响到验证集上的稳定性，换成相加会更加稳定，但是性能会更差。

对于AT_DeltaTIM模型，在MODMA_V1数据集中，将TIM的残差连接方式改为相加，增大weight_decay(0.4)可以一定程度上提高性能（训练集98%，验证集95%）

加载预训练模型时，只需要模型结构相同即可，在forward()中进行的操作一般并不太重要。

对于AT_DeltaTIM模型，在IEMOCAP_V2数据集中，将TIM的残差连接方式改为相加，增大weight_decay(0.5)，提高学习率至5e-4可以一定程度上提高性能（训练集96.613%，验证集98.509%）；在IEMOCAP_V1数据集中，在之前的设置基础上将每次学习率改变的步长从25改为30（训练集 98.068%，测试集92.853%）

对于AT_DeltaTIM模型，使用IEMOCAP上预训练的模型，将预训练时使用的相加改为相乘，对于模型而言其实无明显增益（训练集：99.879  验证集：98.590），但是变得更加稳定了（有时会不稳定）

对于TAB使用1x1卷积+2x2卷积+1x1卷积代替两个2x2卷积，性能会有一定的衰减（大概2%），但会会比较稳定。

AT_DeltaTIM_v2相较于AT_DeltaTIM性能更好一点

使用AFT local attention准确率波动幅度大（一开始很不稳定，后面（60个epoch）还行），但是性能还可以。加大weight decay对系统没有明显的效果。

对于所有的TIM变种网络，将原来的1x1卷积加上BatchNorm和Relu会稳定特别多

```python
nn.Sequential(
            nn.Conv1d(in_channels=args.feature_dim, out_channels=args.filters, 		  kernel_size=1, dilation=1, padding=0),
            nn.BatchNorm1d(args.filters),
            nn.ReLU()
        )
```

不加ReLu也行，性能可能会更好一点，但是比加ReLu会稍微不稳定一些。

如果不加BatchNorm，会和原来一样不稳定

当mask的概率为0.2时会一定程度上提高网络性能（不到0.6%），并且有一点点不稳（该结论有待商榷）

AT_TIM的效果比AT_DeltaTIM的效果好......将dilation从8改到6也可以

将Temporal_Aware_Block中的torch.mul换成torch.add后效果挺不错了的。

用DAIC数据集预训练的模型效果甚至不如IEMOCAP数据集预训练的效果

在训练IEMOCAP数据集时还是选择原始的双向相加形式的TIM比较好



| 模型               | 学习率 | weight-decay | gamma | step_size | 训练集        |      | 测试集 |      |      |
| ------------------ | ------ | ------------ | ----- | --------- | ------------- | ---- | ------ | ---- | ---- |
| v-TIM              | 4e-4   | 0.2          | 0.3   | 30        | 99.879 98.872 |      | 98.315 |      |      |
| AT-TIM(AF)         | 4e-4   | 0.2          | 0.3   | 30        | 100 99.154    |      | 98.315 |      |      |
| AT-TIM(MH)         | 4e-4   | 0.2          | 0.3   | 30        | 100  99.013   |      | 97.753 |      |      |
| MultiTIM(add+DIFF) | 4e-4   | 0.2          | 0.3   | 30        | 100  99.154   |      | 99.438 |      |      |
|                    |        |              |       |           |               |      |        |      |      |

差分信号可以帮助去除干扰，以在测试集得到更高的准确率,但在训练初期时由于不太稳定，所以可能会差分除去一些重要部分，导致在验证集上的结果出现较大波动。以上模型的dilation均设置为6，MultiTIM的num_layer=[3,3]

AT-TIM的TAB残差连接为mul的效果更好

TIM的TAB残差连接为mul的效果更差，但是改为ADD后太好了...

对于Transformer_DeltaTIM而言，残差连接为mul的效果会更好

对于AT-DeltaTIM而言，残差连接为mul的效果会更好，而且会比较稳定。

还是用mul吧，感觉效果会更好一点

AFT注意力可以考虑用AFT_Simple

ADD+Conv的效果感觉不如ADD+DIFF

ADD+SEAT的效果有时好像很好，但有时不太好

SEAT+SEAT似乎还行，但总是验证集上准确率高，测试集上准确率低，总而言之，效果一般

| 模型                     | 学习率 | weight-decay | gamma | step_size | 训练集        |      | 测试集 | 时间                |      |
| ------------------------ | ------ | ------------ | ----- | --------- | ------------- | ---- | ------ | ------------------- | ---- |
| TIM                      | 4e-4   | 0.2          | 0.3   | 30        | 99.879 98.872 |      | 98.312 | 2023 04 16 15:36:54 |      |
| AT-TIM(AF)               | 4e-4   | 0.2          | 0.3   | 30        | 99.950 98.450 |      | 98.875 | 2023 04 16 15:48:29 |      |
| Transformer_DeltaTIM     | 4e-4   | 0.2          | 0.3   | 30        | 100 99.718    |      | 99.578 | 2023 04 16 17:33:56 |      |
| MultiTIM(add+DIFF) [4,4] | 4e-4   | 0.2          | 0.3   | 30        | 100  99.154   |      | 99.156 | 2023 04 16 20:12:40 |      |
|                          |        |              |       |           |               |      |        |                     |      |
|                          |        |              |       |           |               |      |        |                     |      |
|                          |        |              |       |           |               |      |        |                     |      |
|                          |        |              |       |           |               |      |        |                     |      |

用了Multi_train_2_modified_2中的算法，能将测试集上的准确率提到99.719%，不过可能是凑巧，真的是凑巧...

时间感知块的Inception形式效果不好...

注意力机制应该加在提取特征的部分，而不是最后特征融合的部分。

最终决定用AT(AT_TAB)+DIFF的结构，对于AT+DIFF的特征融合，可以采用Conv,pool,linear三种形式，Linear似乎比较稳定一些，注意力选择传统的多头注意力(d_model=128, n_head=4)，选择AFT_local的效果可能更好，但是相比会不太稳定，AFT_simple的效果更不稳定了，有时也可以取得不错的效果，通道注意力的效果不太好。 TAB中残差连接为mul的效果更好。

AT(AT_TAB)+BiDIFF的效果也不错，不过感觉没必要增加计算复杂度

TAB_AT(双向相加后再加注意力)的效果不好。

TAB_simple的效果有点不太稳定，效果也不太好

用CASIA预训练（预训练结果为100%）的模型，冻结通用特征提取器，在MODMA数据集上进行实验，最后在测试集上的准确率为96.062%。给通用特征提取器以更小的学习率的效果为98.594%。

Multitrain_ae：使用在CASIA上预训练的通用特征提取器，在MODMA上训练模型，每隔8个epoch在分别使用CASIA上预训练的通用特征提取器和MODMA上训练的通用特征提取器提取通用特征，通过一个共同的隐编码模型将CASIA分布域和MODMA分布域迁移到中间域上，并对中间域的输出计算MMD损失和域分类损失。

用半监督迁移学习？使用CASIA数据集和一部分MODMA数据集的数据，使用Multitrain_ae训练通用特征提取器，再使用一部分MODMA数据集的数据训练特定特征提取器：说句实话不太行

将Multitrain_ae中的mmd loss作为正则化损失，似乎不太可行

对于迁移学习需要更深的感受野吗？

对于MultiTIM，需要使用IEMOCAP的V2形式数据，否则效果会很差

给音频做归一化的效果不好

对于DDG来说，将mmd loss用在特定特征提取器后（没有用到DANN），可能会有更好的结果

MTCN中加入prepare对于预训练比较重要，加入prepare的预训练结果相比没有加入prepare的效果好一点，对于TCN，加入prepare的效果会好不少。

对于联合训练，加和不加prepare效果差不多

训练CASIA时需要注意使用V2格式、merge最后加入BatchNorm也会影响验证集准确率忽高忽低，需要将step_size改小一点，让模型及时收敛。

Wav2Vec2Processor只是进行补零、归一化等操作。

消融实验：

>  AT+Vanilla  test acc(final): [99.437, 98.875, 99.156]
>
> Vanilla+Vanilla test acc(final): [98.453, 98.453, 98.594]
>
> Vanilla+DIFF test acc(final): [98.453, 99.156, 98.594]

预训练（冻结参数）

> 97.046 97.187 97.750

预训练（微调参数）

> 99.156 98.875 98.734 98.734

联合训练

> （修改分类器）99.156 98.734 99.156
>
> (原始) 99.297 99.156 99.156 
>
> （new）99.156 99.297 98.875

ddg4

> 99.578 99.578 99.297

 
