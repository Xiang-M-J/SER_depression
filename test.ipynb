{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class C:\n",
    "    def __init__(self,x,y,z):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "    def print(self):\n",
    "        for i in self.__dict__:\n",
    "            print(f\"{i}: {self.__dict__[i]}\")\n",
    "\n",
    "c = C(1,2,3)\n",
    "c.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import Args\n",
    "arg = Args()\n",
    "arg.write()\n",
    "arg.mode = \"train\"\n",
    "print(\"修改后：\")\n",
    "print(arg.addition())\n",
    "arg.load(\"hyperparameter.npy\")\n",
    "print(\"修改前：\")\n",
    "print(arg.addition())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import plot_2\n",
    "plot_2(\"results/data/pretrain_drop1_mfcc_smoothTrue_epoch100_l2re1_lr0002_train_metric.npy\", \"now\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(\"pretrain_model.pt\")\n",
    "model_dict = model.state_dict()\n",
    "# for k, v in model_dict.items():\n",
    "#     print(k)\n",
    "\n",
    "model_ = torch.load(\"models/model_drop1_mfcc_smoothTrue_epoch50_l2re1_lr0001_best.pt\")\n",
    "model_dict_ = model_.state_dict()\n",
    "# for k, v in model_dict_.items():\n",
    "#     print(k)\n",
    "pretrained_dict = {k.replace(\"extractor\", \"generalFeatureExtractor\"): v for k, v in model_dict.items() if k.replace(\"extractor\", \"generalFeatureExtractor\") in model_dict_}\n",
    "for k, v in pretrained_dict.items():\n",
    "    print(k)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load(\"preprocess/npy/MODMA_16kHz_fastV3_order2.npy\", allow_pickle=True).item()\n",
    "x = data['x']\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from test_files import test_\n",
    "test_(\n",
    "    model_path=\"models/train_order3_drop1_mfcc_smoothTrue_epoch60_l2re1_lr0002_pretrainTrue_best.pt\", wav_path=\"preprocess/MODMA_16kHz\", num=100, num_class=2, order=3)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "files = np.load(\"sample_wav.npy\")\n",
    "# expect_length = 160000\n",
    "# stack_data = None\n",
    "# for wav in files:\n",
    "#     data, sr = torchaudio.load(wav)\n",
    "#     actual_length = data.shape[1]\n",
    "#     if actual_length < expect_length:\n",
    "#         data = torch.cat([data, torch.zeros([1, expect_length - actual_length])], dim=1)\n",
    "#     elif actual_length > expect_length:\n",
    "#         data = data[:, :expect_length]\n",
    "#     if stack_data is None:\n",
    "#         stack_data = data\n",
    "#     else:\n",
    "#         stack_data = torch.cat([stack_data, data], dim=0)\n",
    "# stack_data = stack_data.numpy().astype(np.float32)\n",
    "# mfcc = librosa.feature.mfcc(y=stack_data, sr=16000, n_mfcc=13, n_fft=800)\n",
    "# mfcc_delta = librosa.feature.delta(mfcc, width=3)\n",
    "# mfcc_acc = librosa.feature.delta(mfcc_delta, width=3)\n",
    "# mfcc = np.concatenate([mfcc, mfcc_delta, mfcc_acc], axis=1).astype(np.float32)\n",
    "\n",
    "mfcc_ = np.load(\"mfcc.npy\").astype(np.float32)\n",
    "labels = np.load(\"labels.npy\")\n",
    "data = np.load(\"preprocess/npy/MODMA_16kHz_fastV2_order3.npy\", allow_pickle=True).item()\n",
    "print(data['y'])\n",
    "for i in range(100):\n",
    "    print(f\"{files[i]}:{labels[i]}\")\n",
    "# print(np.max(np.abs(mfcc_-mfcc)))\n",
    "# print(np.min(np.abs(mfcc_)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import smooth_labels\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "a = torch.rand([4, 4])\n",
    "b = torch.Tensor([[1.,0.,0.,0.],\n",
    "                  [0.,1.,0.,0.],\n",
    "                  [0.,0.,1.,0.],\n",
    "                  [0.,1.,0.,0.]])\n",
    "loss1 = F.cross_entropy(a,b,label_smoothing=0.1)\n",
    "loss2 = F.cross_entropy(a, smooth_labels(b, 0.1))\n",
    "print(loss1)\n",
    "print(loss2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import get_newest_file\n",
    "print(get_newest_file(\"models\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class simplemodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simplemodel, self).__init__()\n",
    "        self.share_net = nn.Linear(4, 4, bias=False)\n",
    "        self.fc1 = nn.Linear(4,2, bias=False)\n",
    "        self.fc2 = nn.Linear(4,2, bias=False)\n",
    "    def forward(self, x, index):\n",
    "        x = self.share_net(x)\n",
    "        if index == 1:\n",
    "            x = self.fc1(x)\n",
    "        else:\n",
    "            x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "x1 = torch.rand([4,4])\n",
    "x2 = torch.rand([4,4])\n",
    "y1 = torch.rand([4,2])\n",
    "y2 = torch.rand([4,2])\n",
    "model = simplemodel()\n",
    "# optimizer1 = torch.optim.Adam(\n",
    "#     [{'params': model.share_net.parameters(), 'lr': 0.001},\n",
    "#      {'params': model.fc1.parameters(), 'lr': 2e-3}])\n",
    "# optimizer2 = torch.optim.Adam(\n",
    "#     [{'params': model.share_net.parameters(), 'lr': 0.001},\n",
    "#      {'params': model.fc2.parameters(), 'lr': 2e-3}])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "out1 = model(x1,1)\n",
    "out2 = model(x2,2)\n",
    "print(model.share_net.weight)\n",
    "print(model.fc1.weight)\n",
    "print(model.fc2.weight)\n",
    "loss1 = F.cross_entropy(out1, y1)\n",
    "loss2 = F.cross_entropy(out2, y2)\n",
    "# optimizer1.zero_grad()\n",
    "# loss1.backward()\n",
    "# optimizer1.step()\n",
    "optimizer.zero_grad()\n",
    "loss1.backward()\n",
    "optimizer.step()\n",
    "print(\"loss1: \")\n",
    "print(model.share_net.weight)\n",
    "print(model.fc1.weight)\n",
    "print(model.fc2.weight)\n",
    "# optimizer2.zero_grad()\n",
    "# loss2.backward()\n",
    "# optimizer2.step()\n",
    "optimizer.zero_grad()\n",
    "loss2.backward()\n",
    "optimizer.step()\n",
    "print(\"loss2: \")\n",
    "print(model.share_net.weight)\n",
    "print(model.fc1.weight)\n",
    "print(model.fc2.weight)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import smooth_labels\n",
    "import torch\n",
    "\n",
    "x = torch.rand([100,16])\n",
    "y = torch.rand([100,16])\n",
    "loss1 = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "loss2 = torch.nn.CrossEntropyLoss()\n",
    "print(loss1(x,y))\n",
    "print(loss2(x, smooth_labels(y,0.1)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "lr = 1e-3\n",
    "lrs = []\n",
    "for i in range(100):\n",
    "    lr_ = lr * (1. - i/100)\n",
    "    lrs.append(lr_)\n",
    "\n",
    "plt.plot(lrs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def print_(model):\n",
    "    for name, params in model.named_parameters():\n",
    "        print('-->name:', name)\n",
    "        print('-->para:', params)\n",
    "        print('-->grad_requires:', params.requires_grad)\n",
    "        print('-->grad_value:', params.grad)\n",
    "        print(\"===\")\n",
    "\n",
    "class simpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleModel, self).__init__()\n",
    "        self.l = nn.Linear(4,2, bias=False)\n",
    "    def forward(self,x):\n",
    "        x = self.l(x)\n",
    "        return x\n",
    "\n",
    "def train_step(model, optimizer, x, y):\n",
    "    out = model(x)\n",
    "    loss = torch.nn.CrossEntropyLoss()(out, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "x = torch.rand([4,4])\n",
    "y = torch.rand([4,2])\n",
    "model = simpleModel()\n",
    "print_(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_step(model, optimizer, x,y)\n",
    "print_(model)\n",
    "\n",
    "train_step(model, optimizer, x, y)\n",
    "print_(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops.layers.torch import Rearrange\n",
    "model1 = nn.Sequential(\n",
    "    Rearrange(\"N L C W -> N L (C W)\"),\n",
    "    Rearrange(\"N L C -> N C L\"),\n",
    ")\n",
    "model2 = Rearrange(\"N L C W -> N C (L W)\")\n",
    "x = torch.rand([4,313,39,1])\n",
    "y1 = model1(x)\n",
    "y2 = model2(x)\n",
    "print(torch.sum(torch.abs(y1-y2)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(\"models/SET_official_train_order3_drop1_mfcc_smoothTrue_epoch60_l2re1_lr0008_pretrainFalse.pt\")\n",
    "torch.save(model, \"test.h5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 121)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "wav, sr = librosa.load(\"preprocess/datasets/RAVDESS/anger/03-01-05-01-01-01-01.wav\", sr=None)\n",
    "mfcc = librosa.feature.mfcc(y=wav, sr=sr)\n",
    "print(mfcc.shape)\n",
    "mfcc_mean = np.mean(mfcc.T, axis=0)\n",
    "print(mfcc_mean.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "import torch\n",
    "x = torch.rand([4, 13, 100])\n",
    "y1 = x.view(x.shape[0], -1)\n",
    "y2 = Rearrange(\"N C L -> N (C L)\")(x)\n",
    "print(torch.sum(torch.abs(y1-y2)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32), tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32)]\n",
      "[tensor([ 8,  9, 10, 11, 12, 13, 14, 15], dtype=torch.int32), tensor([ 8,  9, 10, 11, 12, 13, 14, 15], dtype=torch.int32)]\n",
      "[tensor([16, 17, 18, 19], dtype=torch.int32), tensor([16, 17, 18, 19], dtype=torch.int32)]\n",
      "[tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32), tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32)]\n",
      "[tensor([ 8,  9, 10, 11, 12, 13, 14, 15], dtype=torch.int32), tensor([ 8,  9, 10, 11, 12, 13, 14, 15], dtype=torch.int32)]\n",
      "[tensor([16, 17, 18, 19], dtype=torch.int32), tensor([16, 17, 18, 19], dtype=torch.int32)]\n",
      "[tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32), tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32)]\n",
      "[tensor([ 8,  9, 10, 11, 12, 13, 14, 15], dtype=torch.int32), tensor([ 8,  9, 10, 11, 12, 13, 14, 15], dtype=torch.int32)]\n",
      "[tensor([16, 17, 18, 19], dtype=torch.int32), tensor([16, 17, 18, 19], dtype=torch.int32)]\n",
      "[tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32), tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32)]\n"
     ]
    }
   ],
   "source": [
    "from utils import myDataset\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "x = np.arange(20)\n",
    "y = np.arange(20)\n",
    "dataset = myDataset(x, y)\n",
    "loader = torch.utils.data.dataloader.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "train_iter = iter(loader)\n",
    "print(len(loader))\n",
    "for _ in range(10):\n",
    "    try:\n",
    "        print(next(train_iter))\n",
    "    except StopIteration:\n",
    "        train_iter = iter(loader)\n",
    "        print(next(train_iter))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 1.],\n        [0., 1.],\n        [0., 1.],\n        [0., 1.],\n        [0., 1.],\n        [0., 1.],\n        [0., 1.],\n        [0., 1.],\n        [0., 1.],\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.zeros([12, 1]), torch.ones([12, 1])], dim=1).float()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->name: base.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.4263,  0.1611, -0.4996, -0.1533],\n",
      "        [ 0.0296, -0.1874,  0.1809,  0.2908],\n",
      "        [-0.1542,  0.0798, -0.1907,  0.2638],\n",
      "        [ 0.4201,  0.2417, -0.2194, -0.1610]], requires_grad=True)\n",
      "-->grad_requires: True\n",
      "-->grad_value: tensor([[-0.0019, -0.0029, -0.0091, -0.0053],\n",
      "        [ 0.0046, -0.0046, -0.0199, -0.0087],\n",
      "        [ 0.0068,  0.0072,  0.0222,  0.0136],\n",
      "        [ 0.0147,  0.0107,  0.0277,  0.0190]])\n",
      "===\n",
      "-->name: l1.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[ 0.4156,  0.1491, -0.3953, -0.1435],\n",
      "        [ 0.4043,  0.0392, -0.3317, -0.2097],\n",
      "        [ 0.2651, -0.4623, -0.0913,  0.0342]], requires_grad=True)\n",
      "-->grad_requires: True\n",
      "-->grad_value: tensor([[-0.0110,  0.0010, -0.0015,  0.0134],\n",
      "        [ 0.0108, -0.0036,  0.0010, -0.0016],\n",
      "        [ 0.0003,  0.0026,  0.0005, -0.0117]])\n",
      "===\n",
      "-->name: l2.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.3070, -0.1146,  0.0898, -0.3326],\n",
      "        [-0.3804, -0.2387,  0.2852, -0.0746]], requires_grad=True)\n",
      "-->grad_requires: True\n",
      "-->grad_value: tensor([[ 0.0800, -0.0321,  0.0067, -0.0061],\n",
      "        [-0.0800,  0.0321, -0.0067,  0.0061]])\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from utils import print_model\n",
    "class simplemodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simplemodel, self).__init__()\n",
    "        self.base = nn.Linear(4,4, bias=False)\n",
    "        self.l1 = nn.Linear(4,3, bias=False)\n",
    "        self.l2 = nn.Linear(4,2, bias=False)\n",
    "    def forward(self, x, flag):\n",
    "        x = self.base(x)\n",
    "        if flag:\n",
    "            x = self.l1(x)\n",
    "        else:\n",
    "            x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "model = simplemodel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "x = torch.rand([4,4])\n",
    "y1_ = torch.rand([4,3])\n",
    "y2_ = torch.rand([4,2])\n",
    "y1 = model(x, True)\n",
    "y2 = model(x, False)\n",
    "loss1 = torch.nn.CrossEntropyLoss()(y1, y1_)\n",
    "loss2 = torch.nn.CrossEntropyLoss()(y2, y2_)\n",
    "loss = loss1+loss2\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "print_model(model)\n",
    "optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    x = [[1,2,3], [2,3,4], [3,4,5]]\n",
    "    return x\n",
    "x,y,z = test()\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8400000333786011\n",
      "0.881863534450531\n"
     ]
    }
   ],
   "source": [
    "from utils import plot\n",
    "import numpy as np\n",
    "wav2vec2_data = np.load(\"ser_pretrain/results/wav2vec2.npy\", allow_pickle=True).item()\n",
    "# plot(wav2vec2_data, model_name=\"wav2vec2\",result_path=\"ser_pretrain/results/\")\n",
    "print(np.max(wav2vec2_data['val_acc']))\n",
    "print(wav2vec2_data['train_acc'][np.argmax(wav2vec2_data['val_acc'])])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893333375453949\n",
      "0.8976705074310303\n"
     ]
    }
   ],
   "source": [
    "from utils import plot\n",
    "import numpy as np\n",
    "hubert_data = np.load(\"ser_pretrain/results/hubert.npy\", allow_pickle=True).item()\n",
    "# plot(wav2vec2_data, model_name=\"hubert\",result_path=\"ser_pretrain/results/\")\n",
    "print(np.max(hubert_data['val_acc']))\n",
    "print(hubert_data['train_acc'][np.argmax(hubert_data['val_acc'])])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'DANNModel' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_13520\\2637093397.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"models/dann_iemocap_modma_final.pt\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\python3_7\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[0;32m    710\u001B[0m                     \u001B[0mopened_file\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseek\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0morig_position\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    711\u001B[0m                     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjit\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopened_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 712\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0m_load\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopened_zipfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmap_location\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpickle_module\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mpickle_load_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    713\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0m_legacy_load\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopened_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmap_location\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpickle_module\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mpickle_load_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    714\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\python3_7\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36m_load\u001B[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001B[0m\n\u001B[0;32m   1047\u001B[0m     \u001B[0munpickler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mUnpicklerWrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mpickle_load_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1048\u001B[0m     \u001B[0munpickler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpersistent_load\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpersistent_load\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1049\u001B[1;33m     \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0munpickler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1050\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1051\u001B[0m     \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_loaded_sparse_tensors\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\python3_7\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36mfind_class\u001B[1;34m(self, mod_name, name)\u001B[0m\n\u001B[0;32m   1040\u001B[0m                     \u001B[1;32mpass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1041\u001B[0m             \u001B[0mmod_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload_module_mapping\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmod_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmod_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1042\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind_class\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmod_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1043\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1044\u001B[0m     \u001B[1;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: Can't get attribute 'DANNModel' on <module '__main__'>"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
