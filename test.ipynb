{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class C:\n",
    "    def __init__(self,x,y,z):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "    def print(self):\n",
    "        for i in self.__dict__:\n",
    "            print(f\"{i}: {self.__dict__[i]}\")\n",
    "\n",
    "c = C(1,2,3)\n",
    "c.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import Args\n",
    "arg = Args()\n",
    "arg.write()\n",
    "arg.mode = \"train\"\n",
    "print(\"修改后：\")\n",
    "print(arg.addition())\n",
    "arg.load(\"hyperparameter.npy\")\n",
    "print(\"修改前：\")\n",
    "print(arg.addition())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import plot_2\n",
    "plot_2(\"results/data/pretrain_drop1_mfcc_smoothTrue_epoch100_l2re1_lr0002_train_metric.npy\", \"now\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(\"pretrain_model.pt\")\n",
    "model_dict = model.state_dict()\n",
    "# for k, v in model_dict.items():\n",
    "#     print(k)\n",
    "\n",
    "model_ = torch.load(\"models/model_drop1_mfcc_smoothTrue_epoch50_l2re1_lr0001_best.pt\")\n",
    "model_dict_ = model_.state_dict()\n",
    "# for k, v in model_dict_.items():\n",
    "#     print(k)\n",
    "pretrained_dict = {k.replace(\"extractor\", \"generalFeatureExtractor\"): v for k, v in model_dict.items() if k.replace(\"extractor\", \"generalFeatureExtractor\") in model_dict_}\n",
    "for k, v in pretrained_dict.items():\n",
    "    print(k)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load(\"preprocess/npy/MODMA_16kHz_fastV3_order2.npy\", allow_pickle=True).item()\n",
    "x = data['x']\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from test_files import test_\n",
    "test_(\n",
    "    model_path=\"models/train_order3_drop1_mfcc_smoothTrue_epoch60_l2re1_lr0002_pretrainTrue_best.pt\", wav_path=\"preprocess/MODMA_16kHz\", num=100, num_class=2, order=3)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "files = np.load(\"sample_wav.npy\")\n",
    "# expect_length = 160000\n",
    "# stack_data = None\n",
    "# for wav in files:\n",
    "#     data, sr = torchaudio.load(wav)\n",
    "#     actual_length = data.shape[1]\n",
    "#     if actual_length < expect_length:\n",
    "#         data = torch.cat([data, torch.zeros([1, expect_length - actual_length])], dim=1)\n",
    "#     elif actual_length > expect_length:\n",
    "#         data = data[:, :expect_length]\n",
    "#     if stack_data is None:\n",
    "#         stack_data = data\n",
    "#     else:\n",
    "#         stack_data = torch.cat([stack_data, data], dim=0)\n",
    "# stack_data = stack_data.numpy().astype(np.float32)\n",
    "# mfcc = librosa.feature.mfcc(y=stack_data, sr=16000, n_mfcc=13, n_fft=800)\n",
    "# mfcc_delta = librosa.feature.delta(mfcc, width=3)\n",
    "# mfcc_acc = librosa.feature.delta(mfcc_delta, width=3)\n",
    "# mfcc = np.concatenate([mfcc, mfcc_delta, mfcc_acc], axis=1).astype(np.float32)\n",
    "\n",
    "mfcc_ = np.load(\"mfcc.npy\").astype(np.float32)\n",
    "labels = np.load(\"labels.npy\")\n",
    "data = np.load(\"preprocess/npy/MODMA_16kHz_fastV2_order3.npy\", allow_pickle=True).item()\n",
    "print(data['y'])\n",
    "for i in range(100):\n",
    "    print(f\"{files[i]}:{labels[i]}\")\n",
    "# print(np.max(np.abs(mfcc_-mfcc)))\n",
    "# print(np.min(np.abs(mfcc_)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import smooth_labels\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "a = torch.rand([4, 4])\n",
    "b = torch.Tensor([[1.,0.,0.,0.],\n",
    "                  [0.,1.,0.,0.],\n",
    "                  [0.,0.,1.,0.],\n",
    "                  [0.,1.,0.,0.]])\n",
    "loss1 = F.cross_entropy(a,b,label_smoothing=0.1)\n",
    "loss2 = F.cross_entropy(a, smooth_labels(b, 0.1))\n",
    "print(loss1)\n",
    "print(loss2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import get_newest_file\n",
    "print(get_newest_file(\"models\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class simplemodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simplemodel, self).__init__()\n",
    "        self.share_net = nn.Linear(4, 4, bias=False)\n",
    "        self.fc1 = nn.Linear(4,2, bias=False)\n",
    "        self.fc2 = nn.Linear(4,2, bias=False)\n",
    "    def forward(self, x, index):\n",
    "        x = self.share_net(x)\n",
    "        if index == 1:\n",
    "            x = self.fc1(x)\n",
    "        else:\n",
    "            x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "x1 = torch.rand([4,4])\n",
    "x2 = torch.rand([4,4])\n",
    "y1 = torch.rand([4,2])\n",
    "y2 = torch.rand([4,2])\n",
    "model = simplemodel()\n",
    "# optimizer1 = torch.optim.Adam(\n",
    "#     [{'params': model.share_net.parameters(), 'lr': 0.001},\n",
    "#      {'params': model.fc1.parameters(), 'lr': 2e-3}])\n",
    "# optimizer2 = torch.optim.Adam(\n",
    "#     [{'params': model.share_net.parameters(), 'lr': 0.001},\n",
    "#      {'params': model.fc2.parameters(), 'lr': 2e-3}])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "out1 = model(x1,1)\n",
    "out2 = model(x2,2)\n",
    "print(model.share_net.weight)\n",
    "print(model.fc1.weight)\n",
    "print(model.fc2.weight)\n",
    "loss1 = F.cross_entropy(out1, y1)\n",
    "loss2 = F.cross_entropy(out2, y2)\n",
    "# optimizer1.zero_grad()\n",
    "# loss1.backward()\n",
    "# optimizer1.step()\n",
    "optimizer.zero_grad()\n",
    "loss1.backward()\n",
    "optimizer.step()\n",
    "print(\"loss1: \")\n",
    "print(model.share_net.weight)\n",
    "print(model.fc1.weight)\n",
    "print(model.fc2.weight)\n",
    "# optimizer2.zero_grad()\n",
    "# loss2.backward()\n",
    "# optimizer2.step()\n",
    "optimizer.zero_grad()\n",
    "loss2.backward()\n",
    "optimizer.step()\n",
    "print(\"loss2: \")\n",
    "print(model.share_net.weight)\n",
    "print(model.fc1.weight)\n",
    "print(model.fc2.weight)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import smooth_labels\n",
    "import torch\n",
    "\n",
    "x = torch.rand([100,16])\n",
    "y = torch.rand([100,16])\n",
    "loss1 = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "loss2 = torch.nn.CrossEntropyLoss()\n",
    "print(loss1(x,y))\n",
    "print(loss2(x, smooth_labels(y,0.1)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "lr = 1e-3\n",
    "lrs = []\n",
    "for i in range(100):\n",
    "    lr_ = lr * (1. - i/100)\n",
    "    lrs.append(lr_)\n",
    "\n",
    "plt.plot(lrs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def print_(model):\n",
    "    for name, params in model.named_parameters():\n",
    "        print('-->name:', name)\n",
    "        print('-->para:', params)\n",
    "        print('-->grad_requires:', params.requires_grad)\n",
    "        print('-->grad_value:', params.grad)\n",
    "        print(\"===\")\n",
    "\n",
    "class simpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleModel, self).__init__()\n",
    "        self.l = nn.Linear(4,2, bias=False)\n",
    "    def forward(self,x):\n",
    "        x = self.l(x)\n",
    "        return x\n",
    "\n",
    "def train_step(model, optimizer, x, y):\n",
    "    out = model(x)\n",
    "    loss = torch.nn.CrossEntropyLoss()(out, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "x = torch.rand([4,4])\n",
    "y = torch.rand([4,2])\n",
    "model = simpleModel()\n",
    "print_(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_step(model, optimizer, x,y)\n",
    "print_(model)\n",
    "\n",
    "train_step(model, optimizer, x, y)\n",
    "print_(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops.layers.torch import Rearrange\n",
    "model1 = nn.Sequential(\n",
    "    Rearrange(\"N L C W -> N L (C W)\"),\n",
    "    Rearrange(\"N L C -> N C L\"),\n",
    ")\n",
    "model2 = Rearrange(\"N L C W -> N C (L W)\")\n",
    "x = torch.rand([4,313,39,1])\n",
    "y1 = model1(x)\n",
    "y2 = model2(x)\n",
    "print(torch.sum(torch.abs(y1-y2)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(\"models/SET_official_train_order3_drop1_mfcc_smoothTrue_epoch60_l2re1_lr0008_pretrainFalse.pt\")\n",
    "torch.save(model, \"test.h5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "wav, sr = librosa.load(\"preprocess/datasets/RAVDESS/anger/03-01-05-01-01-01-01.wav\", sr=None)\n",
    "mfcc = librosa.feature.mfcc(y=wav, sr=sr)\n",
    "print(mfcc.shape)\n",
    "mfcc_mean = np.mean(mfcc.T, axis=0)\n",
    "print(mfcc_mean.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "import torch\n",
    "x = torch.rand([4, 13, 100])\n",
    "y1 = x.view(x.shape[0], -1)\n",
    "y2 = Rearrange(\"N C L -> N (C L)\")(x)\n",
    "print(torch.sum(torch.abs(y1-y2)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import myDataset\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "x = np.arange(20)\n",
    "y = np.arange(20)\n",
    "dataset = myDataset(x, y)\n",
    "loader = torch.utils.data.dataloader.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "train_iter = iter(loader)\n",
    "print(len(loader))\n",
    "for _ in range(10):\n",
    "    try:\n",
    "        print(next(train_iter))\n",
    "    except StopIteration:\n",
    "        train_iter = iter(loader)\n",
    "        print(next(train_iter))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cat([torch.zeros([12, 1]), torch.ones([12, 1])], dim=1).float()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from utils import print_model\n",
    "class simplemodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simplemodel, self).__init__()\n",
    "        self.base = nn.Linear(4,4, bias=False)\n",
    "        self.l1 = nn.Linear(4,3, bias=False)\n",
    "        self.l2 = nn.Linear(4,2, bias=False)\n",
    "    def forward(self, x, flag):\n",
    "        x = self.base(x)\n",
    "        if flag:\n",
    "            x = self.l1(x)\n",
    "        else:\n",
    "            x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "model = simplemodel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "x = torch.rand([4,4])\n",
    "y1_ = torch.rand([4,3])\n",
    "y2_ = torch.rand([4,2])\n",
    "y1 = model(x, True)\n",
    "y2 = model(x, False)\n",
    "loss1 = torch.nn.CrossEntropyLoss()(y1, y1_)\n",
    "loss2 = torch.nn.CrossEntropyLoss()(y2, y2_)\n",
    "loss = loss1+loss2\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "print_model(model)\n",
    "optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test():\n",
    "    x = [[1,2,3], [2,3,4], [3,4,5]]\n",
    "    return x\n",
    "x,y,z = test()\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import plot\n",
    "import numpy as np\n",
    "wav2vec2_data = np.load(\"ser_pretrain/results/wav2vec2.npy\", allow_pickle=True).item()\n",
    "# plot(wav2vec2_data, model_name=\"wav2vec2\",result_path=\"ser_pretrain/results/\")\n",
    "print(np.max(wav2vec2_data['val_acc']))\n",
    "print(wav2vec2_data['train_acc'][np.argmax(wav2vec2_data['val_acc'])])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import plot\n",
    "import numpy as np\n",
    "hubert_data = np.load(\"ser_pretrain/results/hubert.npy\", allow_pickle=True).item()\n",
    "# plot(wav2vec2_data, model_name=\"hubert\",result_path=\"ser_pretrain/results/\")\n",
    "print(np.max(hubert_data['val_acc']))\n",
    "print(hubert_data['train_acc'][np.argmax(hubert_data['val_acc'])])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_accuracy = [1,2,5,6,7,7,7,10,5,5,5,5,5,5,5,5]\n",
    "from utils import EarlyStopping\n",
    "early_stop = EarlyStopping(5)\n",
    "for i in range(len(val_accuracy)):\n",
    "    print(val_accuracy[i])\n",
    "    if early_stop(val_accuracy[i]):\n",
    "        break\n",
    "print(\"hello\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(\"models/MultiTIM_train_DAIC_order3_drop1_mfcc_epoch20_l2re2_lr0001_pretrainFalse_clusterFalse.pt\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops.layers.torch import Rearrange\n",
    "# merge = nn.Sequential(\n",
    "#             Rearrange(\"N C L H -> N (C L) H\"),\n",
    "#             nn.AdaptiveAvgPool1d(1),\n",
    "#             Rearrange(\"N (C L) -> N C L H\", C=3)\n",
    "#         )\n",
    "x = torch.rand([2,3,4,2])\n",
    "y = Rearrange(\"N C L H -> N (C L) H\")(x)\n",
    "y = nn.AdaptiveAvgPool1d(1)(y)\n",
    "y = Rearrange(\"N (C L) H -> N C L H\", C=3)(y)\n",
    "y1 = torch.mean(x, dim=-1, keepdim=True)\n",
    "print(x)\n",
    "print(y)\n",
    "print(y1-y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T22:25:29.669576Z",
     "end_time": "2023-04-18T22:25:29.677549Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1896)\n",
      "tensor(1.1896)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.rand([8,4])\n",
    "b = torch.rand([8,4])\n",
    "print(torch.sum(a[0]*b[0]))\n",
    "c = torch.sum(torch.mul(a, b),1)\n",
    "print(c[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T23:00:05.108204Z",
     "end_time": "2023-04-24T23:00:05.114182Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5426, 0.9246, 0.0809, 0.0551],\n",
      "        [0.0834, 0.5260, 0.0796, 0.9738]])\n",
      "tensor([[0.3627, 0.8540, 0.8997, 0.8461],\n",
      "        [0.4418, 0.0315, 0.8641, 0.5291]])\n",
      "tensor([[0.5426, 0.9246, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5260, 0.5000, 0.9738]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand([2,4])\n",
    "b = torch.rand([2,4])\n",
    "print(a)\n",
    "print(b)\n",
    "print(torch.max(a,torch.Tensor([0.5])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T23:15:16.533083Z",
     "end_time": "2023-04-24T23:15:16.539060Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8946, 0.5916, 0.7507, 0.2678, 0.1954, 0.5946, 0.7431, 0.3006]],\n",
      "\n",
      "        [[0.8876, 0.7002, 0.8715, 0.6264, 0.5650, 0.4127, 0.5843, 0.6018]],\n",
      "\n",
      "        [[0.0901, 0.0579, 0.2989, 0.4301, 0.4158, 0.6486, 0.5742, 0.2222]],\n",
      "\n",
      "        [[0.5964, 0.6597, 0.0863, 0.0391, 0.7992, 0.8856, 0.3631, 0.1223]]])\n",
      "tensor([[[0.8946, 0.5916, 0.7507, 0.2678, 0.1954, 0.5946]],\n",
      "\n",
      "        [[0.8876, 0.7002, 0.8715, 0.6264, 0.5650, 0.4127]],\n",
      "\n",
      "        [[0.0901, 0.0579, 0.2989, 0.4301, 0.4158, 0.6486]],\n",
      "\n",
      "        [[0.5964, 0.6597, 0.0863, 0.0391, 0.7992, 0.8856]]])\n"
     ]
    }
   ],
   "source": [
    "from blocks import Chomp1d\n",
    "import torch\n",
    "a = torch.rand([4,1, 8])\n",
    "chomp = Chomp1d(2)\n",
    "print(a)\n",
    "print(chomp(a))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T21:25:05.886527Z",
     "end_time": "2023-04-25T21:25:05.891510Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[[-0.1182],\n         [ 0.0419],\n         [-0.0760],\n         ...,\n         [ 0.0296],\n         [ 0.0092],\n         [-0.0201]],\n\n        [[-0.0551],\n         [ 0.0144],\n         [-0.0099],\n         ...,\n         [-0.0441],\n         [ 0.0297],\n         [-0.0252]],\n\n        [[ 0.0667],\n         [-0.1074],\n         [-0.0266],\n         ...,\n         [ 0.0212],\n         [ 0.0653],\n         [-0.0403]],\n\n        ...,\n\n        [[ 0.0232],\n         [-0.1507],\n         [ 0.0529],\n         ...,\n         [-0.0492],\n         [ 0.0477],\n         [ 0.0450]],\n\n        [[-0.0760],\n         [ 0.0849],\n         [-0.1278],\n         ...,\n         [ 0.1212],\n         [-0.0785],\n         [-0.0934]],\n\n        [[ 0.0469],\n         [-0.0865],\n         [ 0.1038],\n         ...,\n         [-0.0500],\n         [ 0.0948],\n         [-0.1029]]], device='cuda:0', requires_grad=True)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "src_model = torch.load(\"models/ddg/MODMA.pt\")\n",
    "k = 'prepare.net.0.weight'\n",
    "ks = k.split('.')\n",
    "name1 = '.'.join(ks[:-2])\n",
    "model_name = \"src_model\"\n",
    "eval(\"src_model\" + \".\" + name1 +\"[\"+ks[-2]+\"]\"+\".\"+ks[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T08:44:27.114049Z",
     "end_time": "2023-04-27T08:44:27.150926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4709, 0.1689],\n",
      "         [0.6355, 0.0106]],\n",
      "\n",
      "        [[0.6106, 0.4690],\n",
      "         [0.4062, 0.2916]]])\n",
      "tensor([[[-0.7071,  0.7071],\n",
      "         [ 0.7071, -0.7071]],\n",
      "\n",
      "        [[ 0.7071,  0.7071],\n",
      "         [-0.7071, -0.7071]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.rand([2,2,2])\n",
    "print(a)\n",
    "b = (a-torch.mean(a, dim=1, keepdim=True))/torch.std(a, dim=1, keepdim=True)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T22:45:15.483467Z",
     "end_time": "2023-04-27T22:45:15.488452Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global a, b, c\n",
    "    a = 10\n",
    "    b = 10\n",
    "    c = 10\n",
    "\n",
    "a = 1\n",
    "b = 1\n",
    "c = 1\n",
    "main()\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T09:42:12.023013Z",
     "end_time": "2023-05-05T09:42:12.026013Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
